---
title: "572-Assignment 4"
author: "Snehal Thakur"
date: "30/10/2019"
output: word_document
---
```{r}
trainData<- read.csv("C:/Users/Snehal Thakur/Desktop/IDS 572/Assignment 4/IMB 623 VMWare- Digital Buyer Journey/Training.csv")

validData<-read.csv("C:/Users/Snehal Thakur/Desktop/IDS 572/Assignment 4/IMB 623 VMWare- Digital Buyer Journey/Validation.csv")

#On going through the variables one by one we found that there are some variables which only have one constant value throughout. And since no variation won't help in making a final decision, we will be removing them in the next few steps.

var.delete<-!names(trainData)%in%c("tgt_first_date_seminar_page_view","ftr_dummy_highest_prodA_edition0","flag_train","tgt_seminar","tgt_eval","days_since_last_prodS_purchase_date","days_since_first_prodS_purchase_date","days_since_last_prodP_purchase_date","days_since_first_prodP_purchase_date","days_since_last_prodX_purchase_date","days_since_first_prodX_purchase_date","prodM_2015_bookings_amount","prodM_2014_bookings_amount","vsan_license_2013_bookings_amount","vsan_2013_bookings_amount","days_bw_prodS_launch_and_purch","days_bw_prodP_launch_and_purch","days_bw_prodX_launch_and_purch","offline_views","tot_prod17_downloads","tot_prod9_downloads","tot_prod8_downloads","tot_prod4_downloads","tot_prod18_eval_reg","tot_prod17_eval_reg","tot_prod16_eval_reg","tot_prod15_eval_reg","tot_prod12_eval_reg","tot_prod11_eval_reg","tot_prod10_eval_reg","tot_prod9_eval_reg","tot_prod8_eval_reg","tot_prod7_eval_reg","tot_prod6_eval_reg","tot_prod5_eval_reg","tot_prod4_eval_reg","tot_prod3_eval_reg","tot_prod2_eval_reg","tot_prod1_eval_reg","tot_prod18_hol_reg","tot_prod17_hol_reg","tot_prod16_hol_reg","tot_prod15_hol_reg","tot_prod13_hol_reg","tot_prod12_hol_reg","tot_prod11_hol_reg","tot_prod9_hol_reg","tot_prod8_hol_reg","tot_prod7_hol_reg","tot_prod6_hol_reg","tot_prod5_hol_reg","tot_prod4_hol_reg","tot_prod2_hol_reg","tot_prod1_hol_reg","tot_prod18_evals_page_views","tot_prod17_evals_page_views","tot_prod16_evals_page_views","tot_prod15_evals_page_views","tot_prod12_evals_page_views","tot_prod10_evals_page_views","tot_prod9_evals_page_views","tot_prod8_evals_page_views","tot_prod6_evals_page_views","tot_prod5_evals_page_views","tot_prod4_evals_page_views","tot_prod2_evals_page_views","tot_prod1_evals_page_views","tot_prod1_evals_page_views","tot_prod17_hol_page_views","tot_prod16_hol_page_views","tot_prod15_hol_page_views","tot_prod12_hol_page_views","tot_prod9_hol_page_views","tot_prod8_hol_page_views","tot_prod6_hol_page_views","tot_prod5_hol_page_views","tot_prod4_hol_page_views","tot_prod1_hol_page_views","tot_prod18_blog_page_views","tot_prod9_blog_page_views","tot_prod6_blog_page_views","tot_prod18_webinar_page_views","tot_prod17_webinar_page_views","tot_prod16_webinar_page_views","tot_prod15_webinar_page_views","tot_prod12_webinar_page_views","tot_prod9_webinar_page_views","tot_prod8_webinar_page_views","tot_prod6_webinar_page_views","tot_prod4_webinar_page_views","tot_prod2_webinar_page_views","tot_prod1_webinar_page_views","tot_prod18_resources_page_views","tot_prod17_resources_page_views","tot_prod16_resources_page_views","tot_prod8_resources_page_views","tot_prod6_resources_page_views","tot_prod1_resources_page_views","tot_prod18_gs_page_views","tot_prod17_gs_page_views","tot_prod16_gs_page_views","tot_prod15_gs_page_views","tot_prod12_gs_page_views","tot_prod9_gs_page_views","tot_prod8_gs_page_views","tot_prod7_gs_page_views","tot_prod6_gs_page_views","tot_prod5_gs_page_views","tot_prod4_gs_page_views","tot_prod1_gs_page_views","tot_prod18_features_page_views","tot_prod17_features_page_views","tot_prod16_features_page_views","tot_prod8_features_page_views","tot_prod6_features_page_views","tot_prod1_features_page_views","tot_prod20_overview_page_views","tot_prod17_overview_page_views","tot_prod16_overview_page_views","tot_prod12_overview_page_views","tot_prod11_overview_page_views","tot_prod9_overview_page_views","tot_prod8_overview_page_views","tot_prod6_overview_page_views","tot_prod4_overview_page_views","tot_lycos_se_page_views","tot_altavista_se_page_views","tot_bing_se_page_views")
trainData=trainData[,var.delete]

#for valid set
var.delete2<-!names(validData)%in%c("tgt_first_date_seminar_page_view","ftr_dummy_highest_prodA_edition0","flag_train","tgt_seminar","tgt_eval","days_since_last_prodS_purchase_date","days_since_first_prodS_purchase_date","days_since_last_prodP_purchase_date","days_since_first_prodP_purchase_date","days_since_last_prodX_purchase_date","days_since_first_prodX_purchase_date","prodM_2015_bookings_amount","prodM_2014_bookings_amount","vsan_license_2013_bookings_amount","vsan_2013_bookings_amount","days_bw_prodS_launch_and_purch","days_bw_prodP_launch_and_purch","days_bw_prodX_launch_and_purch","offline_views","tot_prod17_downloads","tot_prod9_downloads","tot_prod8_downloads","tot_prod4_downloads","tot_prod18_eval_reg","tot_prod17_eval_reg","tot_prod16_eval_reg","tot_prod15_eval_reg","tot_prod12_eval_reg","tot_prod11_eval_reg","tot_prod10_eval_reg","tot_prod9_eval_reg","tot_prod8_eval_reg","tot_prod7_eval_reg","tot_prod6_eval_reg","tot_prod5_eval_reg","tot_prod4_eval_reg","tot_prod3_eval_reg","tot_prod2_eval_reg","tot_prod1_eval_reg","tot_prod18_hol_reg","tot_prod17_hol_reg","tot_prod16_hol_reg","tot_prod15_hol_reg","tot_prod13_hol_reg","tot_prod12_hol_reg","tot_prod11_hol_reg","tot_prod9_hol_reg","tot_prod8_hol_reg","tot_prod7_hol_reg","tot_prod6_hol_reg","tot_prod5_hol_reg","tot_prod4_hol_reg","tot_prod2_hol_reg","tot_prod1_hol_reg","tot_prod18_evals_page_views","tot_prod17_evals_page_views","tot_prod16_evals_page_views","tot_prod15_evals_page_views","tot_prod12_evals_page_views","tot_prod10_evals_page_views","tot_prod9_evals_page_views","tot_prod8_evals_page_views","tot_prod6_evals_page_views","tot_prod5_evals_page_views","tot_prod4_evals_page_views","tot_prod2_evals_page_views","tot_prod1_evals_page_views","tot_prod1_evals_page_views","tot_prod17_hol_page_views","tot_prod16_hol_page_views","tot_prod15_hol_page_views","tot_prod12_hol_page_views","tot_prod9_hol_page_views","tot_prod8_hol_page_views","tot_prod6_hol_page_views","tot_prod5_hol_page_views","tot_prod4_hol_page_views","tot_prod1_hol_page_views","tot_prod18_blog_page_views","tot_prod9_blog_page_views","tot_prod6_blog_page_views","tot_prod18_webinar_page_views","tot_prod17_webinar_page_views","tot_prod16_webinar_page_views","tot_prod15_webinar_page_views","tot_prod12_webinar_page_views","tot_prod9_webinar_page_views","tot_prod8_webinar_page_views","tot_prod6_webinar_page_views","tot_prod4_webinar_page_views","tot_prod2_webinar_page_views","tot_prod1_webinar_page_views","tot_prod18_resources_page_views","tot_prod17_resources_page_views","tot_prod16_resources_page_views","tot_prod8_resources_page_views","tot_prod6_resources_page_views","tot_prod1_resources_page_views","tot_prod18_gs_page_views","tot_prod17_gs_page_views","tot_prod16_gs_page_views","tot_prod15_gs_page_views","tot_prod12_gs_page_views","tot_prod9_gs_page_views","tot_prod8_gs_page_views","tot_prod7_gs_page_views","tot_prod6_gs_page_views","tot_prod5_gs_page_views","tot_prod4_gs_page_views","tot_prod1_gs_page_views","tot_prod18_features_page_views","tot_prod17_features_page_views","tot_prod16_features_page_views","tot_prod8_features_page_views","tot_prod6_features_page_views","tot_prod1_features_page_views","tot_prod20_overview_page_views","tot_prod17_overview_page_views","tot_prod16_overview_page_views","tot_prod12_overview_page_views","tot_prod11_overview_page_views","tot_prod9_overview_page_views","tot_prod8_overview_page_views","tot_prod6_overview_page_views","tot_prod4_overview_page_views","tot_lycos_se_page_views","tot_altavista_se_page_views","tot_bing_se_page_views")
validData=validData[,var.delete2]
```

```{r}
#Using sapply to check if we have missing values
sapply(trainData,function(x)all(is.na(x)))
```
```{r}
#Now since there are variables that gave us "TRUE" as an output for the above query, that means that we have missing values in our data.

#Get proportion of missing values for columns and see if we can ignore them.
colMeans(is.na(trainData))
```
```{r}
#We see that there are some columns (for eg: ftr_first_date_eval_page_view, ftr_first_date_seminar_page_view, ftr_first_date_whitepaper_download)which have more than 70% of it's data as missing. So we will choose to ignore these columns as the data seems incapable of adding any value to our model.
#remove columns with more than or equal to 70% NA values
cleanTrainData<-trainData[, -which(colMeans(is.na(trainData)) > 0.7)]
#for valid set
cleanValidData<-validData[,-which(colMeans(is.na(validData))>0.7)]

#annualsales columns with missing values
#checking for outliers
attach(cleanTrainData)
hist(db_annualsales,main="db_annualsales Variable Histogram",xlab="db_annualsales")
```
```{r}
#Since the historgram clearly shows that the data has outliers let us check the mean and median of the variable db_annualsales
#na.rm parameter is used to strip the data off of NA values
meanAnnualsales<-mean(cleanTrainData$db_annualsales,na.rm = TRUE)
print(meanAnnualsales)
medianAnnualsales<-median(cleanTrainData$db_annualsales,na.rm = TRUE)
print(medianAnnualsales)
```
```{r}
#Replacing with median
for(i in 1:nrow(cleanTrainData)){
  cleanTrainData$db_annualsales[is.na(cleanTrainData$db_annualsales[i])] <- medianAnnualsales
}

#for valid set
for(i in 1:nrow(cleanValidData)){
  cleanValidData$db_annualsales[is.na(cleanValidData$db_annualsales[i])] <- medianAnnualsales
}
```
```{r}
#random forest using ranger package
library(ranger)
library(caret)
library(dplyr)
vmWareRF<-ranger(target~.,data=select_if(cleanTrainData,is.numeric),importance = "impurity_corrected")
```
```{r}
final<-data.frame(select_if(cleanTrainData,is.numeric))
print(vmWareRF)

#find important variables
?importance_pvalues
importance_pvalues(vmWareRF,method="janitza",formula = target~., data=cleanTrainData)
```
```{r}
#storing top 100 variables in varImp
varImp<-data.frame(colnames(final[,-c(439)]),vmWareRF$variable.importance)
colnames(varImp)<-c("column","importance")
varImp<-varImp[order(varImp$importance),][1:100,]

#pick top 100
pick_index<-function(x,data=cleanTrainData)
{
  which(colnames(data)==x)
}
varIndex<-unlist(sapply(varImp[,1],pick_index))
datatop100<-data.frame(cleanTrainData[,varIndex],cleanTrainData$target)

#for valid set
pick_index2<-function(x,data=cleanValidData)
{
  which(colnames(data)==x)
}
varIndex2<-unlist(sapply(varImp[,1],pick_index2))

#for valid
dataForValidtop100<-data.frame(cleanValidData[,varIndex2],cleanValidData$target)
```
```{r}
datatop100[datatop100==9999]<-NA
datatop100<-datatop100[, -which(colMeans(is.na(datatop100)) > 0.7)]
#for valid set
dataForValidtop100[dataForValidtop100==9999]<-NA
dataForValidtop100<-dataForValidtop100[, -which(colMeans(is.na(dataForValidtop100)) > 0.7)]
```
```{r}
library(randomForest)
fit_rndmFrst<-randomForest(datatop100$cleanTrainData.target~.,data=datatop100,ntree=200,importance=TRUE,mtry=10,do.trace=TRUE)
print(fit_rndmFrst)
```
```{r}
#Finding Accuracy
library(caret)
pred_Set<-predict(fit_rndmFrst,newdata = datatop100)
confusionMatrix(pred_Set,as.factor(datatop100$cleanTrainData.target))
```
```{r}
#for valid set
pred_ValidSet<-predict(fit_rndmFrst,newdata = dataForValidtop100,type = "class")
confusionMatrix(pred_ValidSet,as.factor(dataForValidtop100$cleanValidData.target))
```
```{r}
library(glmnet)
library(mltools)
require(data.table)
data_X <- as.data.table(datatop100[,-ncol(datatop100)])

sparse_matrix_x <- sparsify(data_X)

glm_mod <- cv.glmnet(sparse_matrix_x, datatop100$cleanTrainData.target, family= "multinomial",grouped = TRUE,type.measure = "class", nfolds = 5, alpha = 0)

plot(glm_mod$glmnet.fit,xvar="lambda",label=TRUE)
plot(glm_mod$glmnet.fit, "norm", label=TRUE)

lm <- glm_mod$lambda.min
lm
glm_mod_best <- glmnet(sparse_matrix_x, datatop100$cleanTrainData.target, family= "multinomial", alpha = 0, lambda = lm)
print(glm_mod_best)

#predict accuracy
pred_glmnet<-predict(glm_mod_best,newx = sparse_matrix_x,s=lm,type="response")
pred_glmnet

#confusion matrix
predicted <- colnames(pred_glmnet)[apply(pred_glmnet,1,which.max)]
table(predicted, datatop100$cleanTrainData.target)
```
```{r}
#for valid set
data_Test<-as.data.table(dataForValidtop100[,-ncol(dataForValidtop100)])

sparse_matrix_valid_x <- sparsify(data_Test)
#predict accuracy
predValid_glmnet<-predict(glm_mod_best,newx = sparse_matrix_valid_x,s=lm,type="response")

#confusion matrix
predictedValid <- colnames(predValid_glmnet)[apply(predValid_glmnet,1,which.max)]
table(predictedValid, datatop100$cleanTrainData.target)
```


